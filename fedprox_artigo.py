# -*- coding: utf-8 -*-
"""FEDPROX artigo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jeepSxiyIuIC_o4y1RxDDhsq0-ngilPM

# Importando as bibliotecas
"""

import torchvision
import torch
import torch.nn as nn
from torchvision import datasets, transforms
import tqdm.notebook as tq
import zipfile
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from PIL import Image
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import torch.optim as optim
import copy
from scipy.stats import ks_2samp
from statsmodels.graphics.gofplots import qqplot_2samples
from statsmodels.tsa.stattools import acf
from scipy.optimize import minimize
import time

"""# Serie temporal"""

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/archive.zip"
zip_object = zipfile.ZipFile(file = path, mode = 'r')
zip_object.extractall('./')
zip_object.close()

df_sample = pd.read_csv('/content/sample.csv')
df_train = pd.read_csv('/content/train.csv')
df_test = pd.read_csv('/content/test.csv')

#path = "/content/drive/MyDrive/Traffics.zip"
#path = "/content/drive/MyDrive/Colab Notebooks/Traffics.zip"
#zip_object = zipfile.ZipFile(file = path, mode = 'r')
#zip_object.extractall('./')
#zip_object.close()

#traffic = pd.read_csv('/content/drive/MyDrive/Traffics.csv')
#traffic = pd.read_csv('/content/Traffics.csv')

# Filtrar usuários que estão usando 5G
data_5g = df_train[df_train['is_5g'] == 1]

# Selecionar apenas a coluna `total_flux`
total_flux = data_5g['total_flux']
total_time = data_5g['total_times']

traffic = total_flux/total_time

data_size = int(len(traffic) * 0.2)
traffic_data = total_flux[:data_size]

#traffic_data = traffic['GameBox_DL_Bitrate']

#GameBox_DL_Bitrate = traffic['GameBox_DL_Bitrate']
#GameBox_DL_Bitrate_size = int(len(GameBox_DL_Bitrate) * 0.2)
#traffic_data = GameBox_DL_Bitrate[:GameBox_DL_Bitrate_size]

# Plotar a série temporal
plt.figure(figsize=(10, 6))
plt.plot(traffic_data,linestyle='-')
plt.title('Série Temporal')
plt.xlabel('sec')
plt.ylabel('kbps')
plt.grid(True)
plt.show()

X = traffic_data.to_numpy()
X = np.array(traffic_data)

# Min-Max Scaling
min_value = np.min(X)
max_value = np.max(X)
scaled_serie = (X- min_value) / (max_value - min_value)

# Preparação dos dados em forma sequencial
def prepare_data(seq, n_steps):
    X, y = [], []
    for i in range(len(seq)):
        end_ix = i + n_steps
        if end_ix > len(seq)-1:
            break
        seq_x, seq_y = seq[i:end_ix], seq[end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

n_steps = 24
# Criar dados de entrada e saída
X, y = prepare_data(scaled_serie, n_steps)

# Converter para tensores torch
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).float()

# Separar dados em treinamento e teste (80% treinamento, 20% teste)
data_size = int(len(X) * 0.8)
X_train, X_test = X_tensor[:data_size], X_tensor[data_size:]
y_train, y_test = y_tensor[:data_size], y_tensor[data_size:]

def split_data_for_clients(data, num_clients):
    num_samples = len(data)
    samples_per_client = num_samples // num_clients
    client_data = []
    start_index = 0
    for i in range(num_clients):
        end_index = start_index + samples_per_client
        client_data.append(data[start_index:end_index])
        start_index = end_index
    return client_data

# Número de clientes
num_clients = 50

# Dividir os dados entre os clientes
client_data = split_data_for_clients(X_train, num_clients)

"""# Modelo de previsão"""

# Definir a arquitetura do modelo LSTM
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

class GRU(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):
        super(GRU, self).__init__()

       # Define o número de camadas e dos nós em cada camada
        self.num_layers = num_layers
        self.hidden_size= hidden_size

        # GRU camadas
        self.gru = nn.GRU(
            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob
        )

        # Camadas totalmente conectadas
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Inicialização do estado oculto para a primeira entrada com zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size,device=x.device).requires_grad_()
        out, _ = self.gru(x, h0.detach())

        # Reformulando os outputs na forma de (batch_size, seq_length, hidden_size)
        # para que possa caber na camada totalmente conectada
        out = out[:, -1, :]
        out = self.fc(out)
        return out

"""# Modelo de otimização"""

# Parâmetros do problema de otimização
N = num_clients # número de treinadores locais
B = 1e6  # largura de banda total disponível em Hz
c_m = 15  # ciclos por bit
f_m = np.random.uniform(1e9, 1.6e9, N)  #frequências de computação dos treinadores em Hz
p_tr = 1  # potência de transmissão
p_c = 1  # custo de computação por unidade de dado
D_m = np.random.uniform(5e6, 10e6, N)  # dados locais dos treinadores em bits
d_m = 1e6  # dados para comunicação em bits
T_com = 1.0  # tempo de comunicação fixo
epsilon = 0.01  # precisão global desejada
mu = 1.0  # fator de multiplicação para K_epsilon
b_min = 0.1e6  # mínimo fração de largura de banda em Hz
#rho = 0.5  # parâmetro de ponderação entre custo de recursos e tempo de aprendizagem

# Função objetivo
def objective(x, rho):
    a = x[:N]
    b = x[N:2*N]
    theta = x[-2]
    K_l = x[-1]

    # Calcular R_total
    R_cp = np.sum(a * (D_m * c_m) / f_m) * p_c
    R_total = R_cp

    # Calcular T_total
    T_cp = K_l * np.max((a * (D_m * c_m) / f_m) + T_com)
    T_com_total = np.max(d_m / (b * B))
    K_epsilon = mu * np.log(1 / epsilon) / (1 - theta)
    T_total = K_epsilon * (T_cp + T_com_total)

    return (1 - rho) * R_total + rho * T_total

# Restrições
def constraint1(x):
    return x[-2] - 1e-5

def constraint2(x):
    return 1 - x[-2]

def constraint3(x):
    return B - np.sum(x[:N] * x[N:2*N] * B)

def constraint4(x):
    return np.sum(x[N:2*N]) - 1

def constraint5(x):
    return x[N:2*N] - b_min

def constraint6(x):
    return 1 - x[N:2*N]

def constraint7(x):
    a = x[:N]
    b = x[N:2*N]
    return np.max((a * (D_m * c_m) / f_m) + T_com) - np.max(d_m / (b * B))

def constraint8(x):
    theta = x[-2]
    return mu * np.log(1 / epsilon) / (1 - theta) - x[-1]

# Vetores de inicialização
x0 = np.concatenate((np.ones(N), np.ones(N) * (1 / N), [0.5, 10]))

# Limites para as variáveis
bounds = [(0, 1)] * N + [(b_min / B, 1)] * N + [(1e-5, 1), (1, None)]

# Definição das restrições
constraints = [
    {'type': 'ineq', 'fun': constraint1},
    {'type': 'ineq', 'fun': constraint2},
    {'type': 'ineq', 'fun': constraint3},
    {'type': 'eq', 'fun': constraint4},
    {'type': 'ineq', 'fun': constraint5},
    {'type': 'ineq', 'fun': constraint6},
    {'type': 'ineq', 'fun': constraint7},
    {'type': 'eq', 'fun': constraint8}
]

# Função objetivo
def objective(x):
    a = x[:N]
    b = x[N:2*N]
    theta = x[-2]
    K_l = x[-1]

    # Calcular R_total
    R_cp = np.sum(a * (D_m * c_m) / f_m) * p_c
    R_total = R_cp

    # Calcular T_total
    T_cp = K_l * np.max((a * (D_m * c_m) / f_m) + T_com)
    T_com_total = np.max(d_m / (b * B))
    K_epsilon = mu * np.log(1 / epsilon) / (1 - theta)
    T_total = K_epsilon * (T_cp + T_com_total)

    return (1 - rho) * R_total + rho * T_total

# Restrições
def constraint1(x):
    return x[-2] - 1e-5

def constraint2(x):
    return 1 - x[-2]

def constraint3(x):
    return B - np.sum(x[:N] * x[N:2*N] * B)

def constraint4(x):
    return np.sum(x[N:2*N]) - 1

def constraint5(x):
    return x[N:2*N] - b_min

def constraint6(x):
    return 1 - x[N:2*N]

def constraint7(x):
    a = x[:N]
    b = x[N:2*N]
    return np.max((a * (D_m * c_m) / f_m) + T_com) - np.max(d_m / (b * B))

def constraint8(x):
    theta = x[-2]
    return mu * np.log(1 / epsilon) / (1 - theta) - x[-1]

# Vetores de inicialização
x0 = np.concatenate((np.ones(N), np.ones(N) * (1 / N), [0.5, 10]))

# Limites para as variáveis
bounds = [(0, 1)] * N + [(b_min / B, 1)] * N + [(1e-5, 1), (1, None)]

# Definição das restrições
constraints = [
    {'type': 'ineq', 'fun': constraint1},
    {'type': 'ineq', 'fun': constraint2},
    {'type': 'ineq', 'fun': constraint3},
    {'type': 'eq', 'fun': constraint4},
    {'type': 'ineq', 'fun': constraint5},
    {'type': 'ineq', 'fun': constraint6},
    {'type': 'ineq', 'fun': constraint7},
    {'type': 'eq', 'fun': constraint8}
]

"""# Federated learning

"""

#Inicializando o modelo LSTM
input_size = 1
hidden_size = 64
num_layers = 4
output_size = 1
dropout_prob = 0.2

# Inicializando o modelo global
global_model = GRU(input_size, hidden_size, num_layers, output_size,dropout_prob)
global_model_params = [param.data.clone() for param in global_model.parameters()]

# Instanciar modelos LSTM para cada cliente
client_models = [GRU(input_size, hidden_size, num_layers, output_size,dropout_prob) for _ in range(num_clients)]

# Definir otimizador e função de perda
criterion = nn.MSELoss()
optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in client_models]

# Listas para armazenar perda e acurácia por época
losses_per_epoch = []
accuracies_per_epoch = []

# Instanciar modelos LSTM para cada cliente
client_models = [LSTM(input_size, hidden_size, num_layers, output_size) for _ in range(num_clients)]

# Definir otimizador e função de perda
criterion = nn.MSELoss()
optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in client_models]

# Listas para armazenar perda e acurácia por época
losses_per_epoch = []
accuracies_per_epoch = []

# Função de acurácia
def accuracy(y_pred, y_true, threshold):
    # Calcular a diferença absoluta entre as previsões e os valores verdadeiros
    absolute_diff = torch.abs(y_pred - y_true)
    # Calcular a acurácia como a porcentagem de amostras com diferença absoluta abaixo do limiar
    acc = torch.mean((absolute_diff < threshold).float())
    return acc.item()

def fedprox_loss(local_loss, local_model, global_model, mu):
    prox_term = 0.0
    for param_local, param_global in zip(local_model.parameters(), global_model.parameters()):
        prox_term += ((param_local - param_global) ** 2).sum()
    return local_loss + (mu / 2) * prox_term

"""# Treinamento"""

# Listas para armazenar perda, acurácia e custo por época
losses_per_epoch = []
accuracies_per_epoch = []
costs_per_epoch = []
num_clients_per_epoch = []
test_losses_per_epoch = []
test_accuracies_per_epoch = []
times_per_epoch = []

# Treinamento dos modelos para cada cliente usando FedAvg
for epoch in range(200):
    epoch_losses = []
    epoch_accuracies = []
    start_time = time.time()

    # Atualizar os modelos locais
    for i, model in enumerate(client_models):
        model.train()
        optimizer = optimizers[i]
        X_client = client_data[i]
        y_client = y_train[:len(X_client)]  # Supondo que y_train esteja disponível

        optimizer.zero_grad()

        # Ajustar a forma dos dados para (batch_size, sequence_length, input_size)
        X_client = X_client.unsqueeze(-1)  # Adicionar uma dimensão para input_size

        # Forward pass
        output = model(X_client)

        # Calcular perda local
        local_loss = criterion(output, y_client)
        epoch_losses.append(local_loss.item())

        # Calcular acurácia
        acc = accuracy(output, y_client, threshold=0.1)
        epoch_accuracies.append(acc)

        # Backward pass e atualização dos parâmetros
        local_loss.backward()
        optimizer.step()

    # Calcular média da perda e da acurácia por época
    epoch_loss_avg = np.mean(epoch_losses)
    epoch_acc_avg = np.mean(epoch_accuracies)

    # Armazenar perda e acurácia por época
    losses_per_epoch.append(epoch_loss_avg)
    accuracies_per_epoch.append(epoch_acc_avg)

    print(f"Epoch [{epoch+1}/200], Loss: {epoch_loss_avg:.4f}, Accuracy: {epoch_acc_avg:.4f}")

    # Atualizar o modelo global (FedAvg)
    with torch.no_grad():
        for param_global in global_model.parameters():
            param_global.data.zero_()

        for client_model in client_models:
            for param_global, param_client in zip(global_model.parameters(), client_model.parameters()):
                param_global.data.add_(param_client.data / num_clients)

    # Calcular o custo usando a função de otimização

    # Variação de rho
    rhos = np.linspace(0, 1, 11)  # Exemplo de 11 valores de rho de 0 a 1
    for rho in rhos:
      result = minimize(lambda x: objective(x, rho), x0, method='SLSQP', bounds=bounds, constraints=constraints)
      #print(f"rho: {rho}, resultado: {result.fun}, x: {result.x}")
    #solution = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=constraints)
    #x_opt = solution.x
    #obj_opt = solution.fun
      costs_per_epoch.append(result)

    # Calcular tempo de treinamento por época
    epoch_time = time.time() - start_time
    times_per_epoch.append(epoch_time)

print("Custo ao longo do treinamento:", costs_per_epoch)

# Plotar perda por época
plt.figure(figsize=(10, 5))
plt.plot(losses_per_epoch, label='Loss', color='blue')
plt.xlabel('Época')
plt.ylabel('Loss')
plt.title('Treinamento Loss por Epoca')
plt.legend()
plt.grid(True)
plt.show()

# Plotar acurácia por época
plt.figure(figsize=(10, 5))
plt.plot(accuracies_per_epoch, label='Acurácia', color='green')
plt.xlabel('Época')
plt.ylabel('Acurácia')
plt.title('Treinamento Acurácia por Epoca')
plt.legend()
plt.grid(True)
plt.show()

# Plotar acurácia por época
plt.figure(figsize=(10, 5))
plt.plot(costs_per_epoch, label='Custo', color='green')
plt.xlabel('Época')
plt.ylabel('Custo')
plt.legend()
plt.grid(True)
plt.show()

# Plotar acurácia por época
plt.figure(figsize=(10, 5))
plt.plot(times_per_epoch, label='Custo', color='green')
plt.xlabel('Época')
plt.ylabel('Custo')
plt.legend()
plt.grid(True)
plt.show()

"""# Avaliando o modelo

"""